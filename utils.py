# ## å‡½æ•°å·¥å…·
import os
import re
import pandas as pd
import jieba
import jieba.posseg as psg
import cn2an
import setting
logger = setting.set_logger()

# ç”Ÿæˆæ–°è·¯å¾„ï¼Œç”¨æ¥ä¿å­˜ä¿®æ”¹åæ–‡ä»¶
def get_new_path(old_path, new_tag):
    path_list = old_path.rsplit('.', 1)
    path_list[0] += '_'+new_tag
    return '.'.join(path_list)

# å»é™¤å¼‚å¸¸ç¬¦å·æ¸…æ´—
def remove_abnormal_symbols(content):
    res = re.sub('\xb4|\u200b|Ù©|`|Ğ”|Û¶|à¹“|â•°|â•¯|â™¡|â—¦|Ë™|Ã²|á†º|Ã³|â•|â€¢Ì¥Ì|Ë|â€¢Ì€à¥‚|âœ‚|âœª|â–½', '', str(content))
    return res

# å¯Œæ–‡æœ¬æ¸…æ´—
def rich_text_to_normal(content):
    # åˆ é™¤htmlæ–‡æœ¬
    content = re.sub('<[^<]+?>', '', str(content)).replace('\n', '').strip()
    # åˆ é™¤æ–‡æœ¬ä¸­çš„ç½‘å€
    content = re.sub(r'https://[a-zA-Z0-9.?/&=:]*', '', content)  
    content = re.sub(r'http://[a-zA-Z0-9.?/&=:]*', '', content)
    return content

# å»é™¤æ±‰å­—
def remove_chinese_characters(content):
    content = re.sub('[\u4e00-\u9fa5]', '', content).strip()
    return content

# æ—¶é—´é€‰å–
def search_time_string(content):
    match = re.search('\d{4}-\d{1,2}-\d{1,2}\s*\d{0,2}(:\d{1,2})?(:\d{1,2})?(.\d{1,9})?', content)
    if match:
        return pd.Timestamp(match.group())
    else:
        return pd.NA

# æ•°å­—æ¸…æ´—ï¼ˆæ•´æ•°ï¼‰
def get_number_from_string(content):
    if (not content) or pd.isnull(content) or pd.isna(content):
        return 0
    content = ''.join(re.findall(r"é›¶|å£¹|è²³|åƒ|è‚†|ä¼|é™¸|æŸ’|æŒ|ç–|æ‹¾|å¿µ|ä½°|ä»Ÿ|ã€‡|ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|å»¿|ç™¾|åƒ|ä¸‡|äº¿|\d+",str(content)))
    try:
        content_num = cn2an.cn2an(content, 'smart')
    except Exception as err:
        logger.debug(f'ğŸ¤¡ERROR!!--->{err=}, {type(err)=}')
        content_num = pd.NA
    return content_num

#åˆ†è¯å·¥å…·
def chinese_word_cut(my_text, min_length_of_word = 2, stop_word_path = None, extra_dictionary_path = None, flag_list = ['n','nz','vn']):
    # è®¾ç½®æ‰©å±•å­—å…¸
    if os.path.exists(extra_dictionary_path):
        jieba.load_userdict(extra_dictionary_path)
        jieba.initialize()
    else:
        logger.info(f'æ‰©å……å­—å…¸ä¸å­˜åœ¨{extra_dictionary_path}')
    # è®¾ç½®åœç”¨è¯å­—å…¸
    if os.path.exists(stop_word_path):
        try:
            stopwords_list_unprocessed = open(stop_word_path,encoding ='utf-8')
            stop_word_list = []
            for line in stopwords_list_unprocessed:
                line = re.sub(u'\n|\\r', '', line)
                stop_word_list.append(line)
        except:
            stop_word_list = []
            logger.debug("error in stop_file")
    else:
        logger.info(f'åœç”¨è¯å…¸ä¸å­˜åœ¨{stop_word_path}')

    #jiebaåˆ†è¯
    word_list = []
    seg_list = psg.cut(my_text)
    for seg_word in seg_list:
        # word = re.sub(u'[^\u4e00-\u9fa5]','',seg_word.word) # å»é™¤è‹±æ–‡
        word = seg_word.word
        is_unfit = False
        for stop_word in stop_word_list:  # this word is stop word
            if stop_word == word or len(word) <  min_length_of_word:     
                    is_unfit = True
                    break
        if (not is_unfit) and seg_word.flag in flag_list:
            word_list.append(word)      
    return (" ").join(word_list)

# æ‰‹åŠ¨å®‰å…¨åˆ†è¯
def safe_cut_words(words_str, seg=' '):
    if not seg:
        return
    if not isinstance(words_str, str):
        logger.error(f'æ‰‹åŠ¨å®‰å…¨åˆ†è¯è¾“å…¥å€¼ä¸æ˜¯strï¼Œè€Œæ˜¯{words_str}')
        words_str = str(words_str)
    words_str = re.sub(f'[{seg}]+','|', words_str).strip('|')
    if not words_str:
        return 
    return words_str.split('|')

# åˆ—è¡¨é‡åˆæ£€æµ‹
def list_detection(list1, list2) -> bool:
    return frozenset(list1).intersection(list2)

# è¯å…¸æ£€æµ‹  è¿”å›æ˜¯å¦å±äºå…¶åˆ†ç±»ï¼Œå¹¶è¿”å›é‡åˆè¯
def dictionary_detection(words_str, dic_df, dictionary_column_name, seg = ' '):
    word_list = safe_cut_words(words_str, seg)
    if not word_list:
        return 0, ''
    
    keywords_list = dic_df[dic_df[dictionary_column_name].notna()][dictionary_column_name].to_list()
    
    coincident_keyword_set = list_detection(word_list, keywords_list)
    
    if len(coincident_keyword_set) > 0:
        return 1, ' '.join(coincident_keyword_set)
    
    return 0, ''

# pandas è·å–åˆ†ç±»ï¼ˆä¸€æ•´åˆ—æ•°æ®çš„å…¨éƒ¨åˆ†ç±»åŠé€‰ä¸­è¯ï¼‰
def get_classification(data_df, data_words_column_name, dic_df, inplace=False):
    res_df = pd.DataFrame()
    for class_name in dic_df.columns: 
        res_df = pd.concat((res_df, apply_and_concat_data_frame(data_df, data_words_column_name, dictionary_detection, args=(dic_df, class_name), column_names=[class_name, class_name+'é€‰ä¸­è¯'])), axis=1)
    if inplace:
        return pd.concat((data_df, res_df), axis=1)
    return res_df

# pandas ä¸€åˆ—ç”Ÿæˆå¤šåˆ—
def apply_and_concat_data_frame(df, field, func, args, column_names, inplace=False):
    if inplace:
        return pd.concat((df, df[field].apply(
            lambda cell: pd.Series(func(cell, *args), index=column_names))), axis=1)
    else:
        return df[field].apply(lambda cell: pd.Series(func(cell, *args), index=column_names))

# pandas æ¸…æ´—ä¸€åˆ—æ•°å­—å¹¶ä¿®æ”¹æ ¼å¼
def clean_number_of_data_frame(df, column_names, inplace=False):
    def helper(df, column_name, inplace=inplace):
        if inplace:
            df[column_name] = df[column_name].apply(get_number_from_string).fillna(0).astype('Int64')
        else:
            return df[column_name].apply(get_number_from_string).fillna(0).astype('Int64')
    if isinstance(column_names, list):
        if inplace:
            for column_name in column_names:
                helper(df, column_name, inplace=True)
        else:
            new_df = pd.DataFrame()
            for column_name in column_names:
                new_df[column_name] = helper(df, column_name)
            return new_df
    else:
        return helper(df, column_names, inplace=inplace)

# pandas åˆ†è¯å‡½æ•°
def get_segmentation_words_from_data_frame(data, content_column_name, new_column_name, inplace=False, min_length_of_word = setting.MinLengthOfWord, stop_word_path = setting.StopWordPath, extra_dictionary_path = setting.ExtraDictionaryPath, flag_list = setting.AllowPOSList):
    min_length_of_word = int(min_length_of_word)
    if inplace:
        data[new_column_name] = data[content_column_name].apply(chinese_word_cut, args=(min_length_of_word, stop_word_path, extra_dictionary_path, flag_list))
    else:
        return data[content_column_name].apply(chinese_word_cut, args=(min_length_of_word, stop_word_path, extra_dictionary_path, flag_list))

if __name__ == "__main__":
    try:
        print(pd.Timestamp('2022-1-18  12 23'))
    except:
        print('error')
    print(search_time_string('2022-1-18  12 23'))