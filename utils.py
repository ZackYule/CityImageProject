# ## å‡½æ•°å·¥å…·
import os
import re
import pandas as pd
import jieba
import jieba.posseg as psg
import cn2an
import setting

logger = setting.set_logger()


# ç”Ÿæˆæ–°è·¯å¾„ï¼Œç”¨æ¥ä¿å­˜ä¿®æ”¹åæ–‡ä»¶
def get_new_path(old_path, new_tag):
    path_list = old_path.rsplit('.', 1)
    path_list[0] += '_' + new_tag
    return '.'.join(path_list)


# å»é™¤å¼‚å¸¸ç¬¦å·æ¸…æ´—
def remove_abnormal_symbols(content):
    res = re.sub('\xb4|\u200b|Ù©|`|Ğ”|Û¶|à¹“|â•°|â•¯|â™¡|â—¦|Ë™|Ã²|á†º|Ã³|â•|â€¢Ì¥Ì|Ë|â€¢Ì€à¥‚|âœ‚|âœª|â–½', '',
                 str(content))
    return res


# å¯Œæ–‡æœ¬æ¸…æ´—
def rich_text_to_normal(content):
    # åˆ é™¤htmlæ–‡æœ¬
    content = re.sub('<[^<]+?>', '', str(content)).replace('\n', '').strip()
    # åˆ é™¤æ–‡æœ¬ä¸­çš„ç½‘å€
    content = re.sub(r'https://[a-zA-Z0-9.?/&=:]*', '', content)
    content = re.sub(r'http://[a-zA-Z0-9.?/&=:]*', '', content)
    return content


# å»é™¤æ±‰å­—
def remove_chinese_characters(content):
    content = re.sub('[\u4e00-\u9fa5]', '', content).strip()
    return content


# æ—¶é—´é€‰å–
def search_time_string(content):
    match = re.search(
        '\d{4}-\d{1,2}-\d{1,2}\s*\d{0,2}(:\d{1,2})?(:\d{1,2})?(\.\d{1,9})?',
        content)
    if match:
        return pd.Timestamp(match.group())
    else:
        return pd.NaT


# æ•°å­—æ¸…æ´—ï¼ˆæ•´æ•°ï¼‰
def get_number_from_string(content):
    if (not content) or pd.isnull(content) or pd.isna(content):
        return 0
    content = ''.join(
        re.findall(
            r"é›¶|å£¹|è²³|åƒ|è‚†|ä¼|é™¸|æŸ’|æŒ|ç–|æ‹¾|å¿µ|ä½°|ä»Ÿ|ã€‡|ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|å»¿|ç™¾|åƒ|ä¸‡|äº¿|\d+",
            str(content)))
    # è§£å†³ä¸Šä¸€æ­¥å¾—åˆ°ç©ºå­—ç¬¦ä¸²å¼•å‘çš„bug toDO: éæœ€ä½³æ–¹æ³•
    if (not content) or pd.isnull(content) or pd.isna(content):
        return 0
    try:
        content_num = cn2an.cn2an(content, 'smart')
    except Exception as err:
        logger.debug(
            f'ğŸ¤¡è¯†åˆ«ä¸­æ–‡æ•°å­—æ—¶ERROR!!--->{err=}, {type(err)=}, content:{content}')
        content_num = pd.NA
    return content_num


# åˆ†è¯å·¥å…·
def chinese_word_cut(my_text,
                     min_length_of_word=2,
                     stop_word_path=None,
                     extra_dictionary_path=None,
                     flag_list=['n', 'nz', 'vn']):
    # è®¾ç½®æ‰©å±•å­—å…¸
    if os.path.exists(extra_dictionary_path):
        jieba.load_userdict(extra_dictionary_path)
        jieba.initialize()
    else:
        logger.info(f'æ‰©å……å­—å…¸ä¸å­˜åœ¨{extra_dictionary_path}')
    # è®¾ç½®åœç”¨è¯å­—å…¸
    if os.path.exists(stop_word_path):
        try:
            stopwords_list_unprocessed = open(stop_word_path, encoding='utf-8')
            stop_word_list = []
            for line in stopwords_list_unprocessed:
                line = re.sub(u'\n|\\r', '', line)
                stop_word_list.append(line)
        except:
            stop_word_list = []
            logger.debug("error in stop_file")
    else:
        logger.info(f'åœç”¨è¯å…¸ä¸å­˜åœ¨{stop_word_path}')

    # jiebaåˆ†è¯
    word_list = []
    seg_list = psg.cut(my_text)
    for seg_word in seg_list:
        # word = re.sub(u'[^\u4e00-\u9fa5]','',seg_word.word) # å»é™¤è‹±æ–‡
        word = seg_word.word
        is_unfit = False
        for stop_word in stop_word_list:  # this word is stop word
            if stop_word == word or len(word) < min_length_of_word:
                is_unfit = True
                break
        if (not is_unfit) and seg_word.flag in flag_list:
            word_list.append(word)
    return (" ").join(word_list)


# æ‰‹åŠ¨å®‰å…¨åˆ†è¯
def safe_cut_words(words_str, seg=' '):
    if not seg:
        return
    if not isinstance(words_str, str):
        logger.error(f'æ‰‹åŠ¨å®‰å…¨åˆ†è¯è¾“å…¥å€¼ä¸æ˜¯strï¼Œè€Œæ˜¯{words_str}')
        words_str = str(words_str)
    words_str = re.sub(f'[{seg}]+', '|', words_str).strip('|')
    if not words_str:
        return
    return words_str.split('|')


# åˆ—è¡¨é‡åˆæ£€æµ‹
def list_detection(list1, list2) -> bool:
    return frozenset(list1).intersection(list2)


# è¯å…¸æ£€æµ‹  è¿”å›æ˜¯å¦å±äºå…¶åˆ†ç±»ï¼Œå¹¶è¿”å›é‡åˆè¯
def dictionary_detection(words_str, dic_df, dictionary_column_name, seg=' '):
    word_list = safe_cut_words(words_str, seg)
    if not word_list:
        return 0, ''

    keywords_list = dic_df[dic_df[dictionary_column_name].notna(
    )][dictionary_column_name].to_list()

    logger.debug(word_list)
    coincident_keyword_set = list_detection(word_list, keywords_list)

    if len(coincident_keyword_set) > 0:
        return 1, ' '.join(coincident_keyword_set)

    return 0, ''


# ç±»åˆ«æ£€æµ‹  è¿”å›æ˜¯å¦å±äºå…¶åˆ†ç±»ï¼Œå¹¶è¿”å›é‡åˆè¯
def class_detection(words_str, dic_df, seg=' '):
    for class_name in dic_df.columns:
        is_class, target_word = dictionary_detection(words_str,
                                                     dic_df,
                                                     class_name,
                                                     seg=' ')
        if is_class:
            return class_name
    return ''


# æœ‰æ•ˆå…³é”®è¯æ£€æµ‹
def choose_first_keyword_in_dictionary(words_str,
                                       dictionary_comparison_table,
                                       seg=' '):
    word_list = safe_cut_words(words_str, seg)
    if not word_list:
        return ''
    for word in word_list:
        if word in dictionary_comparison_table.values:
            return word
    return ''


# pandas è·å–åˆ†ç±»ï¼ˆä¸€æ•´åˆ—æ•°æ®çš„å…¨éƒ¨åˆ†ç±»åŠé€‰ä¸­è¯ï¼‰
def get_classification(data_df, data_words_column_name, dic_df, inplace=False):
    res_df = pd.DataFrame()
    for class_name in dic_df.columns:
        res_df = pd.concat(
            (res_df,
             apply_and_concat_data_frame(
                 data_df,
                 data_words_column_name,
                 dictionary_detection,
                 args=(dic_df, class_name),
                 column_names=[class_name, class_name + 'é€‰ä¸­è¯'])),
            axis=1)
    if inplace:
        return pd.concat((data_df, res_df), axis=1)
    return res_df


# pandas å•æ ‡ç­¾åˆ†ç±»ï¼ˆä¸€æ•´åˆ—æ•°æ®çš„å…¨éƒ¨åˆ†ç±»åŠé€‰ä¸­è¯ï¼‰
def get_single_classification(data_df,
                              data_words_column_name,
                              dic_df,
                              inplace=False):
    res_df = pd.DataFrame()
    res_df[data_words_column_name +
           'åˆ†ç±»'] = data_df[data_words_column_name].apply(class_detection,
                                                         args=(dic_df, ))

    if inplace:
        return pd.concat((data_df, res_df), axis=1)
    return res_df


# pandas ä¸€åˆ—ç”Ÿæˆå¤šåˆ—
def apply_and_concat_data_frame(df,
                                field,
                                func,
                                args,
                                column_names,
                                inplace=False):
    if inplace:
        return pd.concat((df, df[field].apply(
            lambda cell: pd.Series(func(cell, *args), index=column_names))),
                         axis=1)
    else:
        return df[field].apply(
            lambda cell: pd.Series(func(cell, *args), index=column_names))


# pandas æ¸…æ´—ä¸€åˆ—æ•°å­—å¹¶ä¿®æ”¹æ ¼å¼
def clean_number_of_data_frame(df, column_names, inplace=False):

    def helper(df, column_name, inplace=inplace):
        if inplace:
            df[column_name] = df[column_name].apply(
                get_number_from_string).fillna(0).astype('Int64')
        else:
            return df[column_name].apply(get_number_from_string).fillna(
                0).astype('Int64')

    if isinstance(column_names, list):
        if inplace:
            for column_name in column_names:
                helper(df, column_name, inplace=True)
        else:
            new_df = pd.DataFrame()
            for column_name in column_names:
                new_df[column_name] = helper(df, column_name)
            return new_df
    else:
        return helper(df, column_names, inplace=inplace)


# pandas åˆ†è¯å‡½æ•°
def get_segmentation_words_from_data_frame(
        data,
        content_column_name,
        new_column_name,
        inplace=False,
        min_length_of_word=setting.MinLengthOfWord,
        stop_word_path=setting.StopWordPath,
        extra_dictionary_path=setting.ExtraDictionaryPath,
        flag_list=setting.AllowPOSList):
    min_length_of_word = int(min_length_of_word)
    if inplace:
        data[new_column_name] = data[content_column_name].apply(
            chinese_word_cut,
            args=(min_length_of_word, stop_word_path, extra_dictionary_path,
                  flag_list))
    else:
        return data[content_column_name].apply(
            chinese_word_cut,
            args=(min_length_of_word, stop_word_path, extra_dictionary_path,
                  flag_list))


# pandas æ—¶é—´åˆ‡åˆ†
def time_cut(df, time_column_name, freq_count=1, freq_tag='M'):
    # è·å–åˆ‡å‰²æ ‡ç­¾
    if freq_count:
        if freq_count == 1:
            freq_str = freq_tag
        else:
            freq_str = str(freq_count) + freq_tag
    # åˆ¤æ–­åˆ‡å‰²å•ä½ï¼ˆæŒ‰å¤©/æŒ‰æœˆï¼‰
    if freq_tag == 'D':
        bins = pd.date_range(
            start=pd.datetime.date(df['å‘å¸ƒæ—¶é—´'].iloc[0]),
            end=pd.datetime.date(df['å‘å¸ƒæ—¶é—´'].iloc[-1] +
                                 pd.DateOffset(days=freq_count)),
            freq=freq_str)
    elif freq_tag == 'M' or freq_tag == 'MS':
        bins = pd.date_range(
            start=pd.datetime.strftime(df['å‘å¸ƒæ—¶é—´'].iloc[0], '%Y-%m'),
            end=pd.datetime.strftime(
                df['å‘å¸ƒæ—¶é—´'].iloc[-1] + pd.DateOffset(months=freq_count),
                '%Y-%m'),
            freq=freq_str)
        logger.debug(freq_str)
        logger.debug(bins)
    else:
        logger.error(f'åˆ‡åˆ†å•ä½é”™ä½ï¼š{freq_tag}')
        return
    df.loc[:, f'{time_column_name}_group'] = pd.cut(df[time_column_name],
                                                    bins,
                                                    right=False)
    return df


# pandas è·å–åˆ†ç»„ç»Ÿè®¡æ•°ï¼ˆä¸åŒappä¸‹ï¼‰
def get_app_group_counts(df_data,
                         group_name,
                         app_name=None,
                         column_name=None,
                         is_fill_zero=False):
    if app_name:
        res_series = df_data[df_data['å¹³å°'] == app_name].groupby(
            group_name).agg('count').iloc[:, 0].astype('Int64')
    else:
        res_series = df_data.groupby(group_name).agg(
            'count').iloc[:, 0].astype('Int64')
    if column_name:
        res_series.name = column_name
    if is_fill_zero:
        res_series = res_series.fillna('0')
    return res_series


# pandas è·å–å¹³å°ç‚¹èµæ•°åˆ†ç»„ç»Ÿè®¡ï¼ˆæˆ–å…¶ä»–ä»»æ„æ•°å­—ï¼‰
def get_app_sum_counts(df_data,
                       group_name,
                       sum_target_volume_name='ç‚¹èµæ•°',
                       app_name=None):
    if not app_name:
        return df_data.loc[:, [sum_target_volume_name, group_name]].groupby(
            group_name).agg('sum', numeric_only=False).astype('Int64')
    return df_data[df_data['å¹³å°'] ==
                   app_name].loc[:,
                                 [sum_target_volume_name, group_name]].groupby(
                                     group_name).agg(
                                         'sum',
                                         numeric_only=False).astype('Int64')


# pandas è·å–æ€»æ•°ï¼Œç›´æ¥ä¿å­˜è‡³åŸè¡¨ä¸­
def get_class_sum(class_list, target_df, data, field=None, filter_content=''):
    # ä¿å­˜åˆ—å
    target_column = filter_content + 'æ€»æ•°'

    if field and filter_content:
        for column in class_list:
            target_df.loc[column, target_column] = data[
                data[field] == filter_content][column].sum()
    elif not field or not filter_content:
        for column in class_list:
            target_df.loc[column, target_column] = data[column].sum()
    else:
        logger.error(
            f'æ²¡æœ‰æŒ‡å®šè¿‡æ»¤åˆ—åç§°,field:{field},filter_content:{filter_content}')

    target_df[target_column] = target_df[target_column].astype('Int64')
    return target_df


if __name__ == "__main__":
    try:
        print(pd.Timestamp('2022-1-18  12 23'))
    except:
        print('error')
    print(search_time_string('2022-1-18  12 23'))
    DataFilePath = 'data/è¥¿å®‰åŸå¸‚å½¢è±¡æ•°æ®_å…³é”®è¯_å…³é”®è¯é€‰å–.pkl'
    DictionaryFilePath = 'data/è¥¿å®‰åŸå¸‚å½¢è±¡ç¼–ç è¯è¡¨.pkl'
    data = pd.read_pickle(DataFilePath).sample(10).reset_index(drop=True)
    dictionary_comparison_table = pd.read_pickle(DictionaryFilePath)
    print(
        get_single_classification(data,
                                  'è®®é¢˜å…³é”®è¯',
                                  dictionary_comparison_table,
                                  inplace=False))
