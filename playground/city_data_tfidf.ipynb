{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF选取关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预设"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import openpyxl\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "import jieba.analyse as analyse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import cn2an\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtraDictionaryPath = '../setting/dict.txt'\n",
    "StopWordPath = 'set/stopwords.txt'\n",
    "KeywordTopNumber = 10\n",
    "minLengthOfWord = 3\n",
    "DataFilePath = '../data/西安城市形象数据_分词.pkl'\n",
    "IDFFilePath = '../setting/pg_idf.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(DataFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15217                                                      \n",
       "9322      学校 学校 气候 痘痘 中药 头发 试试 女生节 部门 男孩子 卡片 硬生生 拼音 汉字 老...\n",
       "145642                                                   奶茶\n",
       "Name: 分词, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['分词'].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理函数工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单篇文档提取关键词\n",
    "def get_key_word_from_a_doc(content, topK=KeywordTopNumber):\n",
    "    return ' '.join(analyse.extract_tags(content, topK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取总体关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# tfidf_vect = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None, max_features=500, max_df = 0.05,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#                                 min_df = 15)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tfidf_vect \u001b[39m=\u001b[39m TfidfVectorizer(use_idf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, smooth_idf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, max_features\u001b[39m=\u001b[39m\u001b[39m1500\u001b[39m, max_df \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                 min_df \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tfidf_features \u001b[39m=\u001b[39m tfidf_vect\u001b[39m.\u001b[39;49mfit_transform(corpus)\n\u001b[1;32m      7\u001b[0m keywords \u001b[39m=\u001b[39m tfidf_vect\u001b[39m.\u001b[39mget_feature_names()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/NLP_Basic/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2078\u001b[0m )\n\u001b[0;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/NLP_Basic/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/NLP_Basic/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1207\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1205\u001b[0m values \u001b[39m=\u001b[39m _make_int_array()\n\u001b[1;32m   1206\u001b[0m indptr\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "corpus = [].append(data['内容'][2])\n",
    "# tfidf_vect = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None, max_features=500, max_df = 0.05,\n",
    "#                                 min_df = 15)\n",
    "tfidf_vect = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None, max_features=1500, max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tfidf_features = tfidf_vect.fit_transform(corpus)\n",
    "keywords = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取总体idf表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4s店</td>\n",
       "      <td>7.933063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一体</td>\n",
       "      <td>7.600288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一家人</td>\n",
       "      <td>7.294906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>一带</td>\n",
       "      <td>6.721738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一楼</td>\n",
       "      <td>7.478398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>鼻尖</td>\n",
       "      <td>7.548730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>鼻翼</td>\n",
       "      <td>7.487810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>鼻部</td>\n",
       "      <td>7.882693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>龅牙</td>\n",
       "      <td>6.123022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>龙首</td>\n",
       "      <td>7.703300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word       idf\n",
       "0     4s店  7.933063\n",
       "1      一体  7.600288\n",
       "2     一家人  7.294906\n",
       "3      一带  6.721738\n",
       "4      一楼  7.478398\n",
       "...   ...       ...\n",
       "1495   鼻尖  7.548730\n",
       "1496   鼻翼  7.487810\n",
       "1497   鼻部  7.882693\n",
       "1498   龅牙  6.123022\n",
       "1499   龙首  7.703300\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# idf_df_data = {'word':keywords}\n",
    "idf_df_data = {'word':keywords,'idf':tfidf_vect.idf_}\n",
    "idf_df = pd.DataFrame(idf_df_data)\n",
    "display(idf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存idf表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df.to_csv(IDFFilePath, header=None, index=None, sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取个体关键字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse.set_idf_path(IDFFilePath);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('就是', 0.24088880000000001),\n",
       " ('好吃', 0.2159506919416227),\n",
       " ('西安', 0.18715726634940635),\n",
       " ('真的', 0.18715726634940635),\n",
       " ('可以', 0.1439671279610818),\n",
       " ('这家', 0.10077698957275726),\n",
       " ('但是', 0.10077698957275726),\n",
       " ('因为', 0.08638027677664908),\n",
       " ('一个', 0.0719835639805409),\n",
       " ('现在', 0.05758685118443272)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = analyse.extract_tags(data['内容'][2], 10, withWeight=True)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好吃\n",
      "0\n",
      "西安\n",
      "0\n",
      "真的\n",
      "0\n",
      "可以\n",
      "0\n",
      "这家\n",
      "0\n",
      "但是\n",
      "0\n",
      "因为\n",
      "0\n",
      "一个\n",
      "0\n",
      "现在\n",
      "0\n",
      "就是\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "    print(idf_df[idf_df['word']==keyword].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['二词关键词'] = data['内容'].apply(get_key_word_from_a_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = analyse.TFIDF(idf_path=IDFFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('就是', 0.24088880000000001),\n",
       " ('好吃', 0.2159506919416227),\n",
       " ('西安', 0.18715726634940635),\n",
       " ('真的', 0.18715726634940635),\n",
       " ('可以', 0.1439671279610818),\n",
       " ('这家', 0.10077698957275726),\n",
       " ('但是', 0.10077698957275726),\n",
       " ('因为', 0.08638027677664908),\n",
       " ('一个', 0.0719835639805409),\n",
       " ('现在', 0.05758685118443272)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.extract_tags(data['内容'][2],topK= 10, withWeight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据展示&保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_show = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47869    摄影 ## 守风人 凌安 DearWhite 不凡 亲爱 西安\n",
       "55437                               小康家庭\n",
       "376       秦岭 葛牌 石船 山沟 龙王庙 沟口 褚家 粉房 沟内 支沟\n",
       "Name: 二词关键词, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_to_show.sample(3)['二词关键词'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = get_new_path(DataFilePath, 'tfidf分词')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(new_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('NLP_Basic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d27efa6b584b4c383956c6770d9f739e2ba2492e6938348fad3a5aa6f8302d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
